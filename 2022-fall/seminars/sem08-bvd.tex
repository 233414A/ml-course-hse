\documentclass[12pt,fleqn]{article}
\usepackage{../lecture-notes/vkCourseML}
\usepackage{lipsum}
\usepackage{indentfirst}
\title{Машинное обучение, ФКН ВШЭ\\Семинар №8}
\author{}
\date{}

\begin{document}

\maketitle

\section{Разложение на смещение и разброс}

На лекции была выведено следующая формула, показывающая, как можно представить ошибку алгоритма регрессии в виде суммы трех компонент:
\[
L(\mu) = 
\EE_{x, y}\bigl[\EE_{X}\bigl[ (y - \mu(X)(x))^2 \bigr]\bigr] = 
    \]
    \[
\underbrace{\EE_{x, y}\bigl[(y - \EE[y|x] )^2\bigr]}_{\text{шум}} + \underbrace{\EE_{x}\bigl[(\EE_{X}[\mu(X)(x)] - \EE[y|x] )^2\bigr]}_{\text{смещение}} +
\underbrace{\EE_{x}\bigl[\EE_{X}\bigl[(\mu(X)(x) - \EE_{X}[\mu(X)(x)] )^2\bigr]\bigr]}_{\text{разброс}},
\]
\begin{itemize}
    \item $\mu(X)$ --- алгоритм, обученный по выборке $X = \{(x_1, y_1), \dots (x_\ell, y_\ell)\}$;
    \item $\mu(X)(x)$ --- ответ алгоритма, обученного по выборке $X$, на объекте $x$;
    \item $\EE_{X}$ --- мат. ожидание по всем возможным выборкам;
    \item $\EE_{X}[\mu(X)(x)]$ --- <<средний>> ответ алгоритма, обученного по всем возможным выборкам $X$, на объекте $x$.
\end{itemize}

С помощью этой формулы мы можем анализировать свойства алгоритма обучения модели $\mu$, если зададим вероятностную модель порождения пар $p(x, y)$.

\subsection{Связь регуляризации с BVD}

Чтобы лучше понять смысл трех компонент, входящих в разложение, рассмотрим одномерную линейную регрессию с $L_2$ регуляризатором.

\paragraph{Алгоритм обучения $\mu$.}  В одномерной линейной регрессии  зависимость целевого признака $y$ от объекта $x$ моделируется с помощью примитивной линейной функции $y = wx$. Оптимальный параметр $w$ находится по выборке $X = \{(x_1, y_1), \dots (x_\ell, y_\ell)\}$ минимизацией 

\[
L(w) = \sum_{i=1}^\ell (y_i - w x_i)^2 + \lambda \cdot w^2. 
\]

При решении данной задачи оптимизации получается следующий алгоритм $\mu(X)$:

\[
\mu(X)(x) = w(X ) \,x, \quad w(X) = \frac{\sum_i x_i y_i}{\sum_i x_i^2 + \lambda}.
\]

Чтобы сделать разложение на смещение и разброс, нужно описать вероятностную модель данных. Это можно делать по-разному. Мы попробуем два набора из предпосылок.

\paragraph{Вероятностная модель №1.} 

Выборка $X$ составляется из $\ell$ независимых пар $(x_i, y_i)$. Будем считать, что объекты $x$ детерминированы, то есть это не случайные величины. Такая предпосылка упростит нам вычисления. Во второй модели мы от неё откажемся. 

Бывают ли объекты $x$ детерминированы в реальной жизни? Иногда бывают. Например, если мы анализируем цены на квартиры, мы можем собрать выборку из $100$ квартир площадью $30$~м$^2$, $100$ площадью $35$~м$^2$ и $100$ площадью $40$~м$^2.$ Если мы соберём другую выборку из трёхсот квартир с такими же площадями, то значения $x$ останутся прежними, а значения $y$ поменяются. 

Правильный ответ на объекте $x$ определяется зашумлённой функцией $f(x)$: $y = f(x) + \varepsilon$. Будем предполагать, что шум пришёл к нам из какого-то распределения с нулевым средним и дисперсией $\sigma^2$. Запишем это как $\varepsilon \sim (0, \sigma^2).$  Иными словами, $y \sim (f(x), \sigma^2).$ 

\begin{vkProblem}
Пусть истинная модель порождения данных выглядит как $f(x) = ax$. Найдите шумовую компоненту для одномерной линейной регрессии с $L_2$ регуляризатором. 
\end{vkProblem}
\begin{esSolution}
    Вероятностная модель позволяет нам довольно легко найти математическое ожидание шумовой компоненты
    \[
    \EE[y|x] = f(x).
    \]  
    Тогда
    \[
    \EE_{x, y}\bigl[(y - \EE[y|x] )^2\bigr] = 
    \EE_{\varepsilon}\bigl[(f(x) + \varepsilon - f(x) )^2\bigr] = 
    \EE_{\varepsilon} (\varepsilon^2) = Var(\varepsilon) + \bigl(\EE\varepsilon\bigr)^2 = \sigma^2 + 0 = \sigma^2.
    \]
\end{esSolution}

\begin{vkProblem}
Найдите смещение алгоритма одномерной линейной регрессии с $L_2$ регуляризатором для $f(x) = ax$.
\end{vkProblem}
\begin{esSolution}
    Для начала найдем <<средний>> по всем выборкам ответ алгоритма на объекте $x$:
    \[
    \EE_{X}[\mu(X)(x)] = \EE_{X}[w(X)] \, x.
    \]
    Помним, что все $x$ фиксированы и их смело можно выносить за математическое ожидание. Случайность в вычислениях есть только из-за ошибки $\varepsilon$
    \begin{multline}
        \label{k_eq1}
        \EE_{X}[w(X)] = \EE_{\varepsilon} \left[\frac{\sum_i x_i (f(x_i) + \varepsilon_i)}{\sum_i x_i^2 + \lambda} \right] = \EE_{\varepsilon} \left[\frac{\sum_i x_i (ax_i + \varepsilon_i)}{\sum_i x_i^2 + \lambda} \right] = \\ =  a \cdot \frac{\sum_i x_i^2}{\sum_i x_i^2 + \lambda} + \frac{\sum_i x_i \cdot \EE(\varepsilon_i)}{\sum_i x_i^2 + \lambda} = a \cdot \frac{\sum_i x_i^2}{\sum_i x_i^2 + \lambda}.
    \end{multline}

    Найдем смещение
    \[
    \EE_{x} \left[\EE_{X}[\mu(X)(x)] - \EE[y|x]  \right] =  \left[a \cdot \frac{\sum_i x_i^2}{\sum_i x_i^2 + \lambda} \cdot x -  a\, x \right] = [\gamma\, a\, x -  a\, x ].
    \]

    Мы сняли математическое ожидание $\EE_{x},$ так как считаем объекты детерминированными. За $\gamma$ мы обозначили величину $\frac{\sum_i x_i^2}{\sum_i x_i^2 + \lambda}$.  
    
    Если $\gamma = 1,$ тогда $\lambda = 0$ и смещение модели равно нулю. В этом случае мы оптимизируем \textit{MSE} без регуляризатора и имеем дело с обычной линейной регрессией. Если $\lambda > 0,$ тогда в модели появляется смещение. При этом смещёнными оказываются не только прогнозы модели, но и коэффициенты в ней. Найдём квадрат смещения, именно его мы будем использовать в разложении

    \[
    \EE_{x} \left[(\EE_{X}[\mu(X)(x)] - \EE[y|x])^2 \right] = (\gamma - 1)^2\, a^2\, x^2
    \]
\end{esSolution}

\begin{vkProblem}
Найдите разброс алгоритма одномерной линейной регрессии с $L_2$ регуляризатором для $f(x) = ax$. 
\end{vkProblem}
\begin{esSolution}
    Нам надо найти \[ \EE_{x}[Var_X(\mu(X)(x))] =  \EE_{x} [\EE_{X} [(\mu(X)(x) - \EE_{X}[\mu(X)(x)] )^2 ]].\]

    Найдём дисперсию алгоритма по обучающей выборке 
    \begin{multline*}
        Var_X(\mu(X)(x)) =  Var_X \left( \frac{\sum_i x_i y_i}{\sum_i x_i^2 + \lambda} x \right) = Var_X \left( \frac{\sum_i x_i (f(x_i) + \varepsilon_i)}{\sum_i x_i^2 + \lambda} x \right) = \\ = Var_X \left( \frac{\sum_i a \, x_i^2}{\sum_i x_i^2 + \lambda} \, x  + \frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2 + \lambda} x \right) = \\ = Var_X  \left(\frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2 + \lambda} x \right) = \left(\frac{\sum_i x_i}{\sum_i x_i^2 + \lambda}\right)^2 x^2 \sigma^2 = \gamma^2 \, x^2 \, \sigma^2
    \end{multline*}

    Получается, что 
    \[
    \EE_{x}[Var_X(\mu(X)(x))] = \EE_{x} [\EE_{X} [(\mu(X)(x) - \EE_{X}[\mu(X)(x)] )^2 ]] = \EE_{x}[\gamma^2 \, x^2 \, \sigma^2] = \gamma^2 \, x^2 \, \sigma^2.
    \]

    Последнее математическое ожидание мы сняли, так как предполагаем, что $x$ детерминированы. 
\end{esSolution}

Вспомним, что $\gamma = \frac{\sum_i x_i^2}{\sum_i x_i^2 + \lambda}$ и запишем получившееся разложение

$$
\EE_{x, y}\bigl[\EE_{X}\bigl[ (y - \mu(X)(x))^2 \bigr]\bigr] = \sigma^2 + (\gamma - 1)^2 a^2 x^2 + \gamma^2 x^2 \sigma^2.
$$

С помощью гиперпараметра $\lambda$ мы можем настраивать силу регуляризации. Чем больше это значение, тем меньше значение$\gamma$. Чем меньше $\gamma,$ тем меньше разброс модели и больше смещение. 

\begin{vkProblem}
    Подберите $\gamma,$ которое будет давать нам оптимальный баланс между смещением и разбросом. Это значение должно улучшать качество прогноза с точки зрения среднеквадратичной ошибки. Правда ли, что $\gamma$ отличается от единицы? Почему силу регуляризации нельзя подобрать по выборке, воспользовавшись получившейся формулой?  
\end{vkProblem}
\begin{esSolution}
    Решим задачу оптимизации 
    
    \[
    \sigma^2 + (\gamma - 1)^2 a^2 x^2 + \gamma^2 x^2 \sigma^2 \to \min_{\gamma}.
    \]
    
    Перед нами парабола с ветвями, смотрящими вверх
    \[ 
    (a^2 + \sigma^2) \, x^2 \, \gamma^2 - 2a^2x^2 \, \gamma + 1 + \sigma^2,
    \]
    значит её минимум можно найти как
    \[
    \gamma^{*} = \frac{2a^2x^2}{2(a^2 + \sigma^2)x^2} = \frac{a^2}{a^2 + \sigma^2}. 
    \]
    Получившаяся величина отличается от единицы, так как $\sigma^2 > 0.$ Выходит, что модель, дающая лучший прогноз, оказывается смещённой. Выразим из $\gamma^{*}$ оптимальное~$\lambda$
    \[
    \lambda^{*} = \frac{\sigma^2}{a^2} \cdot \sum_i x_i^2.
    \]
    Мы не можем подобрать его по обучающей выборке, так как не знаем значения коэффициента $a$. Нам нужно оценить его с помощью нашей модели, оптимизация которой, в свою очередь, зависит от выбранного значения $\lambda.$ Из-за этого приходится подбирать силу регуляризатора с помощью перебора. 
\end{esSolution}

Если в предпосылках у нас будут стохастические признаки $x$, вычисления станут более сложными, но зато результаты станут более общими. 

\subsection{BVD для случайных признаков}

Попробуем проделать всё то же самое для случайных признаков. Для простоты будем работать с линейной моделью без регуляризации. Для неё 

\[
\mu(X)(x) = w(X) \,x, \quad w(X) = \frac{\sum_i x_i y_i}{\sum_i x_i^2}.
\]

\paragraph{Вероятностная модель №2.} Выборка $X$ составляется из $\ell$ независимых пар $(x_i, y_i)$. Будем считать, что объекты $x$ генерируются из нормального распределения $x \sim p(x) = \mathcal{N}(0, \sigma_1^2)$. Правильный ответ на объекте $x$ определяется зашумленной функцией $f(x)$: $y = f(x) + \varepsilon$, $\varepsilon \sim p(\varepsilon) = \mathcal{N}(0, \sigma_2^2)$. Иными словами, $y \sim p(y|x) = \mathcal{N}(f(x), \sigma_2^2)$.

Мы будем рассматривать два простых частных случая: $f(x) = ax$, когда модель зависимости отвечает искомой зависимости, и $f(x)$ --- четная функция, т. е. $f(-x) = f(x)$.  

\begin{vkProblem}
Найдите шумовую компоненту для одномерной линейной регрессии.
\end{vkProblem}
\begin{esSolution}
    Вычисления почти никак не изменятся. Так как распределение $p(y|x)$ нормальное, для него легко вычислить мат. ожидание:
    \[
    \EE[y|x] = f(x).
    \]  
    Тогда
    \[
    \EE_{x, y}\bigl[(y - \EE[y|x] )^2\bigr] = 
    \EE_{x, \varepsilon}\bigl[(f(x) + \varepsilon - f(x) )^2\bigr] = 
    \EE_{\varepsilon} (\varepsilon^2) = Var_{\varepsilon}(\varepsilon) + \bigl(\EE\varepsilon\bigr)^2 = \sigma_2^2 + 0 = \sigma_2^2.
    \]
\end{esSolution}

\begin{vkProblem}
    Найдите смещение алгоритма одномерной линейной регрессии для $f(x) = ax$ и для произвольной четной $f(x)$. 
\end{vkProblem}
\begin{esSolution}
    Для начала найдем <<средний>> по всем выборкам ответ алгоритма на объекте $x$:
    \[
    \EE_{X}[\mu(X)(x)] = \EE_{X}[w(X)] \, x.
    \]
    Итак, нам нужно найти <<среднее>> по всем выборкам значение коэффициента $w$:
    
    \begin{equation}
    \label{k_eq}
    \EE_{X}[w(X)] = \int \frac{\sum_i x_i (f(x_i)+\varepsilon_i)}{\sum_i x_i^2} \prod_i \bigl( p(x_i) p(\varepsilon_i) \bigr) \, dx_1 \dots dx_\ell \, d\varepsilon_1 \dots d\varepsilon_\ell.
    \end{equation}
    Здесь записан несобственный интеграл, в котором каждая переменная принимает значения от $-\infty$ до $\infty$. Значение этого интеграла определяется функцией $f(x)$ и не всегда вычисляется аналитически. В случае, когда истинная зависимость в данных линейная, мы получим:
    \begin{multline*}
    \EE_{X}[w(X)] =  \int \frac{\sum_i x_i (a\,x_i+\varepsilon_i)}{\sum_i x_i^2}  p(\bar x) p(\bar \varepsilon)  d \bar x d \bar \varepsilon = \\ = a \int \frac{\sum_i x_i^2}{\sum_i x_i^2}  p(\bar x)  p(\bar \varepsilon)  d \bar x d \bar \varepsilon +
    \int \frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2}  p(\bar x)  p(\bar \varepsilon)  d \bar x d \bar \varepsilon.
    \end{multline*}
    Мы сократили обозначение для дифференциалов и для плотностей распределений. Первый интеграл равен $a$ (интеграл по всему пространству от плотности распределения). Второй интеграл берется по симметричным относительно нуля интервалам от нечетной по $x_i$ и по $\varepsilon_i$ функции, а значит, равен 0. Итак, <<средний>> коэффициент равен $a$.

    Те же вычисления можно сделать более просто с помощью закона полного математического ожидания $\EE(X) = \EE(\EE(X \mid Y))$ без интеграла

    \begin{multline*} 
    \EE_{X}[w(X)] =  \EE_{X} \left[  \frac{\sum_i x_i (a\,x_i+\varepsilon_i)}{\sum_i x_i^2} \right] = a + \EE_{X} \left[  \frac{\sum_i x_i \, \varepsilon_i}{\sum_i x_i^2} \right] =  a + \EE_{X} \left[\EE_{\varepsilon} \left[ \frac{\sum_i x_i \, \varepsilon_i}{\sum_i x_i^2} \mid x\right] \right] = \\ = a + \EE_{X} \left[\frac{\sum_i x_i \, \EE_{\varepsilon} [\varepsilon_i \mid x]}{\sum_i x_i^2} \right] = a + \EE_{X} \left[\frac{\sum_i x_i \, \EE_{\varepsilon} [\varepsilon_i]}{\sum_i x_i^2} \right] = a.
    \end{multline*}
        
    Найдем смещение:
    \[
    \EE_{x}\bigl[(\EE_{X}[\mu(X)(x)] - \EE[y|x] )^2\bigr] = \EE_{x}\bigl[(a\,x -  a\, x )^2\bigr] = 0.
    \]
    Это интуитивно понятный результат: логично, что перебрав все возможные выборки длины $\ell$ и усреднив по ним значение $k$, мы обязательно найдем истинную величину коэффициента.
    
    Теперь найдем <<среднее>> $k$ для произвольной четной  $f(x)$. По аналогии с предыдущим случаем:
    \[
    \EE_{X}[w(X)] = 
    \int \frac{\sum_i ( x_i f(x_i)) }{\sum_i x_i^2}  p(\bar x)  d\bar x  +
    \int \frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2}  p(\bar x)  p(\bar \varepsilon)  d \bar x d \bar \varepsilon.
    \]  
    Как мы уже выяснили, второй интеграл равен 0. Первый интеграл тоже равен 0, так как подынтегральное выражение --- это нечетная по всем $x_i$ функция. Итак, <<среднее>> значение коэффициента равно нулю. И это тоже понятный результат, потому что четную функцию логично приближать четной, а единственная четная линейная функция, проходящая через 0 --- это $y = 0$.
    
    Найдем смещение:
    \[
    \EE_{x}\bigl[(\EE_{X}[\mu(X)(x)] - \EE[y|x] )^2\bigr] = \EE_{x}\bigl[(0 -  f(x))^2\bigr] = 
    \EE_x f^2(x).
    \]
    Чем меньше четная функция $y=f(x)$ похожа на четную линейную $y = 0$, тем больше будет смещение. Таким образом, если мы пытаемся приблизить нелинейную функцию $f(x)$ в классе линейных, мы получаем большое смещение. Обратите внимание, что если бы $f(x)$ не была четной, мы бы не смогли просто аналитически вычислить интегралы.  
\end{esSolution}

\begin{vkProblem}
    Найдите разброс алгоритма одномерной линейной регрессии для $f(x) = ax$ и для произвольной четной $f(x)$. 
\end{vkProblem}
\begin{esSolution}
    Для $f(x) = ax:$
    \[
    \EE_{x}\bigl[\EE_{X}\bigl[(\mu(X)(x) - \EE_{X}[\mu(X)(x)] )^2\bigr]\bigr] = 
    \EE_{x}\bigl[\EE_{X}\bigl[(
    a x +  \frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2} x - ax
     )^2\bigr]\bigr] =
     \]
     \[ =\biggl( \EE_{x} x^2 \biggr) 
     \biggl (\EE_{X}\biggl(
     \frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2} 
    \biggr )^2\biggr) = \sigma_1^2
    \EE_{X}
    \biggl (\frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2} \biggr)
    ^2.
    \]
    Мат. ожидание можно немного упростить, раскрыв квадрат суммы в числителе и внеся внутрь суммы мат. ожидание по $\bar \varepsilon = (\varepsilon_1, \dots, \varepsilon_\ell)$:
    \[
    \EE_{X}\biggl(
    \frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2} 
    \biggr )^2 = \EE_{\bar x} \biggl[ \frac {\sum_{i \ne j} x_i x_j \EE_{\bar \varepsilon } [\varepsilon_i \varepsilon_j] +
        \sum_i x_i^2 \EE_{\bar \varepsilon } \varepsilon_i^2 } {(\sum_i x_i^2)^2} 
     \biggr].
    \]
    Так как $\varepsilon_i$ и $\varepsilon_j$ независимы, $\EE[\varepsilon_i \varepsilon_j] = 0$, а $\EE_{\bar \varepsilon } \varepsilon_i^2 = \sigma_2^2$. Тогда
    \[
    \EE_{X}\biggl(
    \frac{\sum_i x_i \varepsilon_i}{\sum_i x_i^2} 
    \biggr )^2 = \EE_{\bar x} \biggl[ \frac {
        \sum_i x_i^2 \sigma_2^2} {(\sum_i x_i^2)^2} 
    \biggr] = \sigma_2^2 \EE_{\bar x} \biggl[ \frac 1 {\sum_i x_i^2} \biggr],
    \]
    а разброс
    \[
    \EE_{x}\bigl[\EE_{X}\bigl[(\mu(X)(x) - \EE_{X}[\mu(X)(x)] )^2\bigr]\bigr] = 
     \sigma_1^2 \sigma_2^2
    \EE_{\bar x} \biggl[ \frac 1 {\sum_i x_i^2} \biggr].
    \]
    Последнее мат. ожидание также можно рассчитать, но мы не будем этого делать. Мы получили, что если шум в ответах небольшой, то и разброс модели будет небольшой. 
    
    Для четной $f(x)$:
    \[
    \EE_{x}\bigl[\EE_{X}\bigl[(\mu(X)(x) - \EE_{X}[\mu(X)(x)] )^2\bigr]\bigr] = 
    \EE_{x}\bigl[\EE_{X}\bigl[( 0 -
    \frac{\sum_i x_i (f(x_i)+\varepsilon_i)}{\sum_i x_i^2} x
    )^2\bigr]\bigr] =
    \]
    \[ =\biggl( \EE_{x} x^2 \biggr) 
    \biggl (\EE_{X}\biggl(
    \frac{\sum_i x_i (f(x_i)+\varepsilon_i)}{\sum_i x_i^2}
    \biggr )^2\biggr) = \sigma_1^2
    \EE_{X}\biggl[
    \frac{\sum_i x_i (f(x_i)+\varepsilon_i)}{\sum_i x_i^2}
    \biggr ]^2.
    \]
\end{esSolution}

Для линейной модели с регуляризатором можно проделать всё то же самое, но интегралы будут более сложными. 

\subsection{Разложение для решающих деревьев}

Мы выяснили, что линейные модели имеют маленькое смещение, когда истинная зависимость в данных также линейна, и большое смещение, если это не так.
С решающими деревьями ситуация противоположна. Мы не будем формально это обосновывать, но интуитивно понятно, что поскольку для любой выборки можно построить дерево, имеющую нулевую ошибку на обучении, то смещение решающего дерева будет небольшое для любой истинной зависимости $f(x)$. Разброс, наоборот, будет большой, потому что при малом изменении в выборке мы можем получить совершенно другое решающее дерево. 

С другой стороны, можно ограничивать многообразие деревьев, установив ограничение на глубину или минимальное число объектов в листовых вершинах. Тогда смещение будет увеличиваться, а разброс уменьшаться. 
В граничном случае, когда $\mu(x) = C = \text{const}$, разброс, очевидно, будет равен 0.
    
\subsection{Приближенное вычисление интегралов}
Разложение на смещение и разброс --- это теоретическая конструкция, показывающая, из-за чего происходит переобучние и недообучение алгоритмов. Обычно при анализе сложности алгоритма оперируют терминами <<смещение>> и <<разброс>>, качественно оценивая их величину (например, мы так сделали с деревьями). Вычислить компоненты аналитически для большинства алгоритмов не представляется возможным. Однако, если есть необходимость количественно оценить их, можно воспользоваться техниками приближенного вычисления интегралов с помощью семплирования.

Если нам нужно оценить математическое ожидание $\EE_{x \sim p(x)} f(x) = \int f(x) p(x) dx$, то можно просемплировать выборку $\{x_1, \dots, x_n\}$ из распределения $p(x)$  и приближенно вычислить интеграл:
\[
\EE_{x \sim p(x)} f(x) \approx \frac 1 n \sum_{i=1}^n f(x_i).
\]
Из областей с большим значением плотности в выборку попадет больше точек, и они внесут больший вклад в значение интеграла. Несложно показать, что данная оценка является несмещенной.

Для вычисления математического ожидания по случайным выборкам нужно сгенерировать несколько выборок: 
\[
    \EE_{X}
    [\mu(X)(x)]
    \approx
    \frac{1}{n}
    \sum_{i=1}^n
        \mu(X_i)(x),
        \quad
        X_i=\{(x_{i, 1}, y_{i, 1}), \dots, (x_{i, \ell}, y_{i, \ell})\}, \, x_{i,j}, y_{i, j} \sim p(x, y)
\]

В numpy для генерации выборок можно пользоваться модулем numpy.random. 

\section{Композиционные алгоритмы}
\subsection{Беггинг}
Существуют способы уменьшить разброс алгоритма. Наиболее известный из них --- бэггинг. Беггинг заключается в генерации нескольких новых выборок $X_1, \dots, X_m$ на основе имеющейся, обучении алгоритма на каждой из сгенерированных выборок и усреднении ответов всех алгоритмов на новом объекте.  

\begin{vkProblem}
    При бэггинге новую выборку $\widetilde X$ составляют, генерируя элементы из $X$ с возвращением. При этом объекты в $\widetilde X$ могут повторяться. Будем считать, что число объектов в $\widetilde X$ и в $X$ одинаковое и равно $\ell$. Найдите вероятность того, что конкретный объект попадет в выборку.
    \end{vkProblem}%\vspace{2cm}    
    \begin{esSolution}
        
        Вероятность того, что объект попадет в выборку при одном вытаскивании --- $\frac 1 \ell$, $\ell$~--- число объектов выборки. Вероятность того, что не попадет --- $1 - \frac 1 \ell$; не попадет ни при одном вытаскивании --- 
        $\biggl ( 1 - \frac 1 \ell \biggr)^\ell$. Наконец, искомая вероятность
        \[
        \lim_{\ell \rightarrow \infty} 1 - \biggl ( 1 - \frac 1 \ell \biggr)^\ell =
        1 - \frac{1}{e}.
        \]
    \end{esSolution}
    
    На лекции было показано, что усреднение с помощью бэггинга уменьшает разброс алгоритма в $m$ раз, если базовые алгоритмы мало коррелированы. 
    
    Такой эффект наблюдается при усреднении предсказаний любых алгоритмов регрессии, необязательно полученных бэггингом, если эти алгоритмы выдают слабо коррелированные ответы.
    
    \subsection{Простое голосование}
    Идею о том, что при выборе итогового предсказания для объекта $x$ с помощью агрегации предсказаний нескольких базовых алгоритмов  предсказание будет точнее, можно применить и к классификации. Наиболее простой способ это сделать --- построить несколько классификаторов, предсказывающих класс для объекта $x$, и выбирать класс, который чаще всего предсказывали эти алгоритмы (простое голосование).
    \begin{vkProblem}
        Пусть у нас есть три бинарных классификатора, каждый из которых ошибается с вероятностью $p$. С какой вероятностью будет ошибаться классификатор, построенный с помощью простого голосования? При каких значениях $p$ эта вероятность будет меньше $p$?
    \end{vkProblem}    
    \begin{esSolution}
        Простое голосование выдаст правильный ответ, если не более чем один алгоритм ошибется. Вероятность того, что все три алгоритма ответят правильно, равна $(1-p)^3$, вероятность того, что ровно два из трех ответят правильно: $3p(1-p)^2$. Итоговая вероятность ошибки:
        \[
        1 - (1-p)^3 - 3p(1-p)^2 = 
        1 - 1 + 3p - 3p^2 + p^3 - 3p + 6 p^2 - 3 p^3 = 
         - 2 p^3+ 3  p^2 = p^2 (3 - 2p).
        \]
        \[
         - 2 p^3+ 3  p^2 < p;
        \]
        \[
        p(- 2 p^2+ 3 p - 1) < 0;
        \]
        \[
        -2 p (p-1) (p - \frac 1 2) < 0.
        \]
        На участке $p \in [0, 1]$ решением этого неравенства является полуинтервал $p \in (0, \frac 1 2)$. Таким образом, если каждый из алгоритмов хотя бы чуть лучше, чем случайный, то композиция будет давать меньше ошибок, чем каждый алгоритм по отдельности.
    \end{esSolution}


    \begin{vkProblem}
        Рассмотрим задачу классификации с выборкой \(X\) и долей объектов с \( y = + 1 \) равной \( 0.5 \).
        Разделим ее на две равные части: \( X_1 \) и \( X_2 \).
        Баланс классов в них равен \( p_1 \) и \( p_2 \) соответственно. Легко показать, что \( p_2 = 1 - p_2 \).
        
        Будем говорить, что \( X_1 \) и \( X_2 \) – это два сегмента нашей выборки. На них используются два
        разных классификатора \( a_1(x) \) и \( a_2(x) \). Модель на всей выборке можно записать так:
        \[
            a(x) = a_1(x)\,[x \in X_1] + a_2(x)\,[x \in X_2]
            \]
        Пусть на каждом сегменте работает случайный классификатор:
        \[
            \text{AUC}\,(a_1) = \text{AUC}\,(a_2) = 0.5
            \]
        Может ли \( \text{AUC}\,(a) \) быть больше? Каким может быть качество модели на всей выборке?
    \end{vkProblem}
    
    \begin{esSolution}
        Воспользуемся вероятностным определением метрики ROC AUC:
        \[
            \text{AUC}\,(a) = \mathbb P \big \{ a(x_i) > a(x_j)\; |\; y_i = +1,\; y_j = -1 \big \}
            \]
        Проведем вероятностный эксперимент: выберем случайный объект положительного класса и случайный объект
        отрицательного класса. Вероятность того, что на первом объекте ответ модели больше, и является
        ROC AUC модели.
        
        Рассмотрим такой вероятностный эксперимент для классификатора \( a \). Случайный \textit{положительный}
        объект принадлежит первому сегменту \( X_1 \) с вероятностью 
        \[
            \frac{p_1}{p_1 + p_2} = \frac{p_1}{p_1 + 1 - p_1} = p_1
            \]
        И с вероятностью \( 1 - p_1 \) – второму сегменту, \( X_2 \).
        
        Аналогично случайный \textit{отрицательный} объект принадлежит первому сегменту \( X_1 \) с вероятностью 
        \[
            \frac{1 - p_1}{1 - p_1 + 1 - p_2} = \frac{1 - p_1}{1 - p_1 +  p_1} = 1 - p_1
            \]
        И с вероятностью \( p_1 \) – второму сегменту, \( X_2 \).
        
        Если и положительный, и отрицательный объект принадлежат \( X_1 \), то
        \[
            \mathbb P \big \{ a(x_i) > a(x_j)\; |\; y_i = +1,\; y_j = -1,\; x_i,\; x_j \in X_1  \big \}
            = \text{AUC}\,(a_1) = 0.5
            \]
        Обозначим эту вероятность \( auc_{11} \). Оба объекта принадлежат первому сегменту
        с вероятностью \( \delta_{11} = p_1 (1 - p_1) \). Аналогично с вероятностью
        \( \delta_{22} = p_1 (1 - p_1) \)
        оба объекта принадлежат \( X_2 \), и \( auc_{22}=0.5 \).
        
        Но что будет, если положительный объект из \( X_1 \), а отрицательный из \( X_2 \)? Предположим, что
        предсказания модели на первом сегменте \textit{всегда больше} предсказаний модели на втором.
        \[
            a_1(x_i) > a_2(x_j)\quad \forall x_i \in X_1,\; x_j \in X_2
            \]
        Тогда \( auc_{12} = 1 \), и это происходит с вероятностью \( \delta_{12} = p_1 ^ 2 \). А с вероятностью
        \( \delta_{21} = (1 - p_1) ^ 2\) получим положительный объект из \( X_2 \) и отрицательный – из \( X_1 \).
        В этом случае \( auc_{21} = 0 \). 
        
        Вычислим \( \text{AUC}\,(a) \) по формуле полной вероятности:
        \[
            \text{AUC}\,(a) = auc_{11}\,\delta_{11} + auc_{12}\,\delta_{12} +
                                        auc_{21}\,\delta_{21} + auc_{22}\,\delta_{22} =
            \]
        \[
            =0.5p_1(1 - p1) + p_1^2 + 0.5p_1(1 - p1) = p_1
            \]
        Получаем, что \( \text{AUC}\,(a) \) \textit{может принимать любые значения от 0 до 1}.
        
        Качество на всей выборке зависит от
        баланса классов на подвыборках после разбиения на сегменты. Если это разбиение случайно
        (\(p_1 = p_2 = 0.5\)), то и качество на всей выборке не будет отличаться от качества на
        отдельных сегментах. Если же разбиение на сегменты \textit{информативно}, то сам этот
        факт может усилить модель.
    \end{esSolution}

\end{document}